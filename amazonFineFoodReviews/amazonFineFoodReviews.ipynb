{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Amazon fine food reviews analysis\n",
    "Data source : https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "\n",
    "## Context\n",
    "This dataset consists of reviews of fine foods from amazon. The data span a period\n",
    "of more than 10 years, including all ~500,000 reviews up to October 2012.\n",
    "Reviews include product and user information, ratings, and a plain text review.\n",
    "It also includes reviews from all other Amazon categories.\n",
    "\n",
    "Data includes:\n",
    "\n",
    "- Reviews from Oct 1999 - Oct 2012\n",
    "- 568,454 reviews\n",
    "- 256,059 users\n",
    "- 74,258 products\n",
    "- 260 users with > 50 reviews\n",
    "\n",
    "Attributes information :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of attributes :  10\n",
      "Id\n",
      "ProductId\n",
      "UserId\n",
      "ProfileName\n",
      "HelpfulnessNumerator\n",
      "HelpfulnessDenominator\n",
      "Score\n",
      "Time\n",
      "Summary\n",
      "Text\n"
     ]
    },
    {
     "data": {
      "text/plain": "(568454, 10)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Reviews.csv')\n",
    "print(\"Total number of attributes : \", len(data.columns))\n",
    "for i in data.columns:\n",
    "    print(str(i))\n",
    "\n",
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "HelpfulnessNumerator - Number of user who found reviews helpful\n",
    "\n",
    "HelpfulnessDenominator - Number of user who indicated whether they found the review helpful or not.\n",
    "\n",
    "## Objective :\n",
    "For a given review determine whether it is positive or negative.\n",
    "\n",
    "(Assumption : Considering 4 and 5 start as positive and 1 and 2 as negative.)\n",
    "\n",
    "For testing purpose we will be using only 1,2,4,5 star rating only.\n",
    "\n",
    "***\n",
    "\n",
    "## Step 1 : Reading Data\n",
    "\n",
    "We will use sqlite for reading data and perform most of the operation using sql commands."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    443777\n",
      "negative     82037\n",
      "Name: Score, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Id   ProductId          UserId                      ProfileName  \\\n0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n\n   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n0                     1                       1  positive  1303862400   \n1                     0                       0  negative  1346976000   \n2                     1                       1  positive  1219017600   \n3                     3                       3  negative  1307923200   \n4                     0                       0  positive  1350777600   \n\n                 Summary                                               Text  \n0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n2  \"Delight\" says it all  This is a confection that has been around a fe...  \n3         Cough Medicine  If you are looking for the secret ingredient i...  \n4            Great taffy  Great taffy at a great price.  There was a wid...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>negative</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>Natalia Corres \"Natalia Corres\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1219017600</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>Karl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>negative</td>\n      <td>1307923200</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>Michael D. Bigham \"M. Wassir\"</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1350777600</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Using sqlite to table to read data\n",
    "\n",
    "# First create a connection between the sqlite database and python\n",
    "con = sqlite3.connect('././database.sqlite')\n",
    "\n",
    "#Discard 3 star rating\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\"SELECT * FROM REVIEWS WHERE SCORE!=3\"\"\",con)\n",
    "\n",
    "# we will map 1 , 2 star rating with negative and  4 , 5 with positive rating.\n",
    "\n",
    "def mapping(n):\n",
    "    if n>3:\n",
    "        return 'positive'\n",
    "    else :\n",
    "        return 'negative'\n",
    "\n",
    "filtered_data['Score'] = filtered_data['Score'].map(mapping)\n",
    "\n",
    "print(filtered_data['Score'].value_counts())\n",
    "filtered_data.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2 : Cleaning of data :\n",
    "\n",
    "For cleaning purpose we will remove some duplicates items from the dataset.\n",
    "We can also remove some ambiguous data like if helpfulnessNumerator is greater than\n",
    "helpfulDenominator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 10)\n"
     ]
    }
   ],
   "source": [
    "# we will sort the data then remove items where time , UserId, ProfileName , HelpfulnessNumerator,HelpfulDenominator , Score , Summary and Text are equal.\n",
    "# We will choose the first data and remove other and also we don't want copy of the data.\n",
    "\n",
    "filtered_data = filtered_data.sort_values(axis=0,ascending=True,inplace=False,by='ProductId')\n",
    "filtered_data.drop_duplicates(subset=filtered_data.columns[2:],keep='first',inplace=False)\n",
    "print(filtered_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# Converting text into vector form\n",
    "\n",
    "For data visualization we need to convert text into vector form so that we can apply\n",
    "some linear algebra technique to find similarity and dissimilarity between two documents.\n",
    "\n",
    "Popular and simple method of feature extraction with text data which are currently used are:\n",
    "- Bag of words (BOW).\n",
    "- TF-IDF\n",
    "- Word2Vec\n",
    "\n",
    "## Text pre-processing :\n",
    "Before applying any text to vector conversion algorithm we need to remove some\n",
    "unimportant and garbage data.\n",
    "There are some important text pre-processing technique are\n",
    "- Removing stopwords .\n",
    "- Converting every letter to small letter.\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Semantic meanings of words\n",
    "\n",
    "### Removing Stopwords :\n",
    "In our english language there are many stopwords available like 'this' , 'that' , 'is' , 'not'\n",
    "etc. these words are places in the sentences to make grammatically correct. They don't have much\n",
    "signification impact on our data.\n",
    "So before applying any algorithm we remove these stopwords from the documents.\n",
    "\n",
    "### Stemming :\n",
    "Stemming a process in which similar looking words with similar meaning are replace with\n",
    "the other text.\n",
    "Like 'tasty' , 'taste','tasteful' etc can be replace with  'taste'.\n",
    "There are many algorithm available to do stemming but two most important stemming algorithm are :\n",
    "- Porter Stemmer.(Old one)\n",
    "- Snowball Stemmer.(New one)\n",
    "\n",
    "### Lemmatization :\n",
    "Breaking a sentences into words.\n",
    "\n",
    "### Semantic Analysis :\n",
    "In this case not same looking words but with similar meaning words can replace with same word.\n",
    "Like tasty and delicious having same meaning so they can be replace with either tasty or delicious.\n",
    "\n",
    "Note -\n",
    "In bag of words and TF-IDF we don't consider Semantic Analysis.\n",
    "\n",
    "\n",
    "## Bag of words(BOW) :\n",
    "In bag of words technique we create a d-dimensions vector for each sentence.\n",
    "Where d is equal to total number of different words.\n",
    "After we find d we create a d-dimensional vector  where each dimension represent count of word of that particular type\n",
    "\n",
    "## TF-IDF :\n",
    "TF - Term frequency\n",
    "$TF(w_j,r_i)$ is equal to total number of time $w_j$ occurs in $r_i$ divided by Total number\n",
    "of words in $r_i$.\n",
    "\n",
    "0<= $TF(w_j,r_i)$ <= 1\n",
    "\n",
    "IDF - Inverse Document frequency\n",
    "\n",
    "Let's say $D_c = { r_1,r_2,r_3, .... , r_n}$\n",
    "where $D_c$ is knows as data corpus and $r_1,r_2,.....,r_n$ are the sentences.\n",
    "\n",
    "$ IDF(w_j,D_c) = log(N/n_i)$ where\n",
    "\n",
    "$IDF(w_j,D_c)$ is idf of word $w_j$ in document\n",
    "\n",
    "N is number of documents $D_c$\n",
    "\n",
    "$n_i$ is the number of documents which contain $w_j$\n",
    "\n",
    "So the vector form of text can be written as  :\n",
    "\n",
    "$TFIDF(w_j,r_i) = TF(w_j,r_i)*IDF(w_j,D_c)$\n",
    "\n",
    "## Word2Vec :\n",
    "Word2vec is a combination of models used to represent distributed representations\n",
    "of words in a corpus C. Word2Vec (W2V) is an algorithm that accepts text corpus\n",
    "as an input and outputs a vector representation for each word.\n",
    "\n",
    "It's math is fairly complex so we will skip this for now but will do in  deep learning model.\n",
    "What it does ?\n",
    "- It's take word as input and return a \"vector representation\" of each word.\n",
    "- If two word $w_1$ and $w_2$ are more similar w.r.t. $w_3$ then it will return vector $v_1,\n",
    "v_2,v_3$ such that distance between $v_1$ and $v_2$ is less w.r.t to $v_3$.\n",
    "\n",
    "But we need to convert a sentence into vector not a word .\n",
    "So convert into sentences we will use two famous method :\n",
    "- Avg Word2Vec.\n",
    "- TFIDF Word2Vec.\n",
    "\n",
    "In avg word2vec we will calculate word2vec for each word and then we will take avg of it's vector.\n",
    "\n",
    "In TFIDF word2vec we will use weighted mean .\n",
    "Weight of each word is equal to IFIDF.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}